{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l-OmQD2TH8O9"
   },
   "outputs": [],
   "source": [
    "# !pip -q install transformers datasets peft accelerate adapter-transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def in_venv():\n",
    "    return sys.prefix != sys.base_prefix\n",
    "\n",
    "in_venv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /h/245/poyuchen/.local/lib/python3.10/site-packages (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft --no-dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd0472f1b024a82bb6869bf9f1a1146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zSzs5--7FQ5I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration boolq-150dc58bba7bf36d\n",
      "Found cached dataset parquet (/u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f774ab98c6fd45a985895a664c6ef2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"boolq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6OU6RKDrFeEb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1e8f8b774871ade.arrow\n",
      "Loading cached processed dataset at /u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9531fa37afdd21a.arrow\n"
     ]
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    question = example['question']\n",
    "    passage = example['passage']\n",
    "    answer = 'Yes' if example['answer'] else 'No'\n",
    "    prompt = f\"Question: {question}\\nPassage: {passage}\\nAnswer:\"\n",
    "    return {'input_text': prompt, 'target_text': answer}\n",
    "formatted_dataset = dataset.map(format_example, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5N7y9nSaFr5h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b3d5c1713fbdc4df.arrow\n",
      "Loading cached processed dataset at /u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dd5decfda74ec72d.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = [f\"{input_text} {target_text}\" for input_text, target_text in zip(examples['input_text'], examples['target_text'])]\n",
    "\n",
    "    # Tokenize the combined text and return PyTorch tensors\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',  # Ensure tensors are returned\n",
    "    )\n",
    "\n",
    "    # Create labels by cloning the input IDs\n",
    "    labels = model_inputs['input_ids'].clone()\n",
    "\n",
    "    # Determine the length of the input_text for each example\n",
    "    input_lengths = []\n",
    "    for input_text in examples['input_text']:\n",
    "        input_ids = tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,  # Do not add special tokens here\n",
    "        )['input_ids']\n",
    "        input_lengths.append(len(input_ids))\n",
    "\n",
    "    # Mask the labels for the input_text portion\n",
    "    for i, input_length in enumerate(input_lengths):\n",
    "        labels[i, :input_length] = -100  # Use -100 to ignore the input tokens\n",
    "\n",
    "    model_inputs['labels'] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zP-ULfLKTbEs"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We are not using masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJoSF7AHGIGN",
    "outputId": "9a5a0889-3c92-4ccf-ba7e-c0f75db77e71"
   },
   "outputs": [],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', load_in_8bit=True, device_map='auto')\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")  \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9HjKHsxpGWHX"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set the pad_token to the eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update model's config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    # evaluation_strategy='steps',\n",
    "    # eval_steps=1000,\n",
    "    save_steps=3000,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    report_to='none',\n",
    ")\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "bh0XGBoQGlV4",
    "outputId": "70414caf-9d6f-4eaa-baa6-79472497da92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23570' max='23570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23570/23570 2:03:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.764400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.608100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.601900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23570, training_loss=1.7284354195738425, metrics={'train_runtime': 7433.2207, 'train_samples_per_second': 12.682, 'train_steps_per_second': 3.171, 'total_flos': 2.8231407828467712e+17, 'train_loss': 1.7284354195738425, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lpmwvU8Lg1W0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration boolq-150dc58bba7bf36d\n",
      "Found cached dataset parquet (/u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /u/poyuchen/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1915d7e2291719be.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the validation split\n",
    "dataset = load_dataset(\"boolq\", split='validation')\n",
    "formatted_dataset = dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9201bdce650c412cbdf35776d3658588",
      "df0ded98c0f244f29d7d3c89a8c24a3b",
      "5d112a12624a4ffaa2b01aee31ef2c65",
      "039c6ad0e769410db2ab65abfbed21b6",
      "ae94d4e9966441abbce82326a2485fb9",
      "eaec53d563f1471c8804b99f010648bd",
      "7c2510a0aa194575963c63572df6e9a7",
      "ee153541b27b47b7b44995f4f1c5fe86",
      "2e57b5f1267f4d07800bff3dcd9e94eb",
      "b527b13061424a16aee1a0cc8adb5ced",
      "4f60072f012b4f8b926dc07d20eb9d1c"
     ]
    },
    "id": "DHmOI7lsg892",
    "outputId": "6f0282bc-3941-4d27-b8d3-07577e5805ee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2b55c84e7b487c9416e83c7481b89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3270 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w/284/poyuchen/MAT1510_Final/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/w/284/poyuchen/MAT1510_Final/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "\n",
    "def generate_answer(example):\n",
    "    input_text = example['input_text']\n",
    "\n",
    "    # Tokenize and move inputs to the model's device\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "\n",
    "    # Generate the model's output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer by removing the input prompt\n",
    "    answer = generated_text[len(input_text):].strip()\n",
    "\n",
    "    # Handle empty outputs\n",
    "    if not answer:\n",
    "        answer = \"No\"\n",
    "        return {'predicted_answer': answer}\n",
    "\n",
    "    # # Keep only the first word (Yes or No)\n",
    "    # answer = answer.split()[0]\n",
    "    # print(answer)\n",
    "\n",
    "    # # Handle unexpected outputs\n",
    "    if 'yes' in answer.lower():\n",
    "        answer = 'Yes'\n",
    "    elif 'no' in answer.lower():\n",
    "        answer = 'No'\n",
    "    else:\n",
    "        answer = 'No'\n",
    "\n",
    "    return {'predicted_answer': answer}\n",
    "\n",
    "\n",
    "# Map the generate_answer function over the validation set\n",
    "predictions = formatted_dataset.map(generate_answer, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvEk1vEMhH5E",
    "outputId": "f678e795-77e1-4708-b978-37ea79fd300b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 44.37%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(predictions):\n",
    "    # Convert the true and predicted answers to lowercase for consistency\n",
    "    true_answers = [answer.lower() for answer in predictions['target_text']]\n",
    "    predicted_answers = [answer.lower() for answer in predictions['predicted_answer']]\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = sum([\n",
    "        true == pred for true, pred in zip(true_answers, predicted_answers)\n",
    "    ])\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = correct_predictions / len(true_answers)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = compute_accuracy(predictions)\n",
    "print(f\"Accuracy on the validation set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "039c6ad0e769410db2ab65abfbed21b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b527b13061424a16aee1a0cc8adb5ced",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4f60072f012b4f8b926dc07d20eb9d1c",
      "value": "â€‡3270/3270â€‡[12:07&lt;00:00,â€‡â€‡4.73â€‡examples/s]"
     }
    },
    "2e57b5f1267f4d07800bff3dcd9e94eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f60072f012b4f8b926dc07d20eb9d1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d112a12624a4ffaa2b01aee31ef2c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee153541b27b47b7b44995f4f1c5fe86",
      "max": 3270,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e57b5f1267f4d07800bff3dcd9e94eb",
      "value": 3270
     }
    },
    "7c2510a0aa194575963c63572df6e9a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9201bdce650c412cbdf35776d3658588": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df0ded98c0f244f29d7d3c89a8c24a3b",
       "IPY_MODEL_5d112a12624a4ffaa2b01aee31ef2c65",
       "IPY_MODEL_039c6ad0e769410db2ab65abfbed21b6"
      ],
      "layout": "IPY_MODEL_ae94d4e9966441abbce82326a2485fb9"
     }
    },
    "ae94d4e9966441abbce82326a2485fb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b527b13061424a16aee1a0cc8adb5ced": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df0ded98c0f244f29d7d3c89a8c24a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaec53d563f1471c8804b99f010648bd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7c2510a0aa194575963c63572df6e9a7",
      "value": "Map:â€‡100%"
     }
    },
    "eaec53d563f1471c8804b99f010648bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee153541b27b47b7b44995f4f1c5fe86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
