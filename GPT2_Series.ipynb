{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c2cac5eb329f437fbfeb0f77d9dd0034":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f68f90d6d2bb41a5b225d4b8b8874daf","IPY_MODEL_7492cba8ee504271b9e4822cbddb20d3","IPY_MODEL_7bac94fc39a44a7a87a1274bfb1e767e"],"layout":"IPY_MODEL_87223e45e70c4a299385ca42c463bd57"}},"f68f90d6d2bb41a5b225d4b8b8874daf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_374fa9b22fa647278c6d733cb91d3492","placeholder":"​","style":"IPY_MODEL_1d521ab9a2194b269cfaaed670d91b52","value":"Downloading readme: 100%"}},"7492cba8ee504271b9e4822cbddb20d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9c8c82b77894de88b7a8ceefeca43b6","max":6570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40b61569fe744cf184e6e5ea4b218427","value":6570}},"7bac94fc39a44a7a87a1274bfb1e767e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9700b3913957485da141baac1e136547","placeholder":"​","style":"IPY_MODEL_daf52d70782c4fefa82a25904b208bb3","value":" 6.57k/6.57k [00:00&lt;00:00, 521kB/s]"}},"87223e45e70c4a299385ca42c463bd57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374fa9b22fa647278c6d733cb91d3492":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d521ab9a2194b269cfaaed670d91b52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9c8c82b77894de88b7a8ceefeca43b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b61569fe744cf184e6e5ea4b218427":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9700b3913957485da141baac1e136547":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daf52d70782c4fefa82a25904b208bb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24d4d6304ceb488dac7f15cd2fac820a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7c0c3ee652a4f4c93492783fbfc7b16","IPY_MODEL_07929a20484745fe89fec46cbf9182a4","IPY_MODEL_d26f7a5b39304e9e89eadde8635345f9"],"layout":"IPY_MODEL_93101fb671ea4cdd9d04984976e2cf95"}},"a7c0c3ee652a4f4c93492783fbfc7b16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_445a319833564a5a97c8002b3a383d0c","placeholder":"​","style":"IPY_MODEL_8cbcbd93970748ac83688ee18993af7b","value":"Downloading data files: 100%"}},"07929a20484745fe89fec46cbf9182a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d460d0f0cad4c19adf462220c1e5c64","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfea3ee0d2d542c0b31fe99bd3daa5ad","value":2}},"d26f7a5b39304e9e89eadde8635345f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b6d3f46f6034e6496e99eb13cab3bae","placeholder":"​","style":"IPY_MODEL_cd5d55f4a5774b6c9ea3f7d33b757b27","value":" 2/2 [00:01&lt;00:00,  1.17it/s]"}},"93101fb671ea4cdd9d04984976e2cf95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"445a319833564a5a97c8002b3a383d0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cbcbd93970748ac83688ee18993af7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d460d0f0cad4c19adf462220c1e5c64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfea3ee0d2d542c0b31fe99bd3daa5ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b6d3f46f6034e6496e99eb13cab3bae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd5d55f4a5774b6c9ea3f7d33b757b27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e60754232204491bad128476a7e9074e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2199d41160f49d0be41225d6c5ebbe7","IPY_MODEL_0e1c266d4b634729be110966111a6249","IPY_MODEL_330903ab409e43b2a4aaf27ba561cd05"],"layout":"IPY_MODEL_150b7d3320164f96a02cd20a60180de7"}},"b2199d41160f49d0be41225d6c5ebbe7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4fb1e9f65994aef892291963ab116ce","placeholder":"​","style":"IPY_MODEL_7d4c415b019b4bfaacf361d866a6267a","value":"Downloading data: 100%"}},"0e1c266d4b634729be110966111a6249":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ad99b08f594e9c8c9cb22368e1c272","max":3685146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97ffd4ce0b8c42fabbdf43cb09b04650","value":3685146}},"330903ab409e43b2a4aaf27ba561cd05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb06ba2b1ab94e218843aaac20ec0e13","placeholder":"​","style":"IPY_MODEL_0a2306db7b5d44498ed2d247b4bad415","value":" 3.69M/3.69M [00:00&lt;00:00, 41.6MB/s]"}},"150b7d3320164f96a02cd20a60180de7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4fb1e9f65994aef892291963ab116ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d4c415b019b4bfaacf361d866a6267a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41ad99b08f594e9c8c9cb22368e1c272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97ffd4ce0b8c42fabbdf43cb09b04650":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb06ba2b1ab94e218843aaac20ec0e13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a2306db7b5d44498ed2d247b4bad415":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a7a8e4782e349b8ad48f90c5dd0ee9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd28806be7f44834b41ae9a2dd25f94b","IPY_MODEL_1e12861d69c044eab02eeeccbdbd4b3e","IPY_MODEL_da597767e7d246cba45862beb0b4ae37"],"layout":"IPY_MODEL_8ac9c5bb8fa24613879e323fd207d8d5"}},"fd28806be7f44834b41ae9a2dd25f94b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_605880ef77a54504ad42cd74c9de26d8","placeholder":"​","style":"IPY_MODEL_53f89ec859a2402d9ea5e695eefd91e4","value":"Downloading data: 100%"}},"1e12861d69c044eab02eeeccbdbd4b3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d5a7b6125b143d1852bb748e7752a02","max":1257630,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8496c3dc441460fb8bc67e958ba4d63","value":1257630}},"da597767e7d246cba45862beb0b4ae37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61efdb12bac24032b1feeb23d6b3ab17","placeholder":"​","style":"IPY_MODEL_b14d7a1a82ee49f38ce2e114004de6b7","value":" 1.26M/1.26M [00:00&lt;00:00, 22.2MB/s]"}},"8ac9c5bb8fa24613879e323fd207d8d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"605880ef77a54504ad42cd74c9de26d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53f89ec859a2402d9ea5e695eefd91e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d5a7b6125b143d1852bb748e7752a02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8496c3dc441460fb8bc67e958ba4d63":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61efdb12bac24032b1feeb23d6b3ab17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b14d7a1a82ee49f38ce2e114004de6b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cffb7ed5b08f4a4aa50bfe878ee88e23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15362b965bfa4af8a3c470b3de8b5c4e","IPY_MODEL_29cb460d11d64a8dbda8c67a75a5dae9","IPY_MODEL_0ea9f64f201f4ac7b978b19fe1836a44"],"layout":"IPY_MODEL_b939c7af30c14a2a918aba53f11bc1de"}},"15362b965bfa4af8a3c470b3de8b5c4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08b80efe5f044d6aaaa9edc3a4bdb366","placeholder":"​","style":"IPY_MODEL_5cec17d289a24f0f94d764d65107af62","value":"Extracting data files: 100%"}},"29cb460d11d64a8dbda8c67a75a5dae9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_610cba62284c4aeeb85a012fc94e25bf","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed04740e7ded484aba77bb4552ef5834","value":2}},"0ea9f64f201f4ac7b978b19fe1836a44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aeb904d9deb74586a7e65e64765edbf0","placeholder":"​","style":"IPY_MODEL_da16f11a6db64bee9357435122e697ed","value":" 2/2 [00:00&lt;00:00, 141.55it/s]"}},"b939c7af30c14a2a918aba53f11bc1de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b80efe5f044d6aaaa9edc3a4bdb366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cec17d289a24f0f94d764d65107af62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"610cba62284c4aeeb85a012fc94e25bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed04740e7ded484aba77bb4552ef5834":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aeb904d9deb74586a7e65e64765edbf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da16f11a6db64bee9357435122e697ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a488d8bad4c49babd73ef0a4bc94f99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bc659f2cbe7498284bcc7cca5825e34","IPY_MODEL_ef1839c460df4a2e972f3495e08328ff","IPY_MODEL_ecc8bf1944114aecbd7e324c8c2bf507"],"layout":"IPY_MODEL_c4360706c2424953a8f3d30b4f2f4210"}},"2bc659f2cbe7498284bcc7cca5825e34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bfce7e56cbe4b41989e65c5d0c246bd","placeholder":"​","style":"IPY_MODEL_de298a7d016f4596a89942986a286e3d","value":""}},"ef1839c460df4a2e972f3495e08328ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8bfe741382344678206ad2fb906d812","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_683957c70e2549579951d57fc7f8bba7","value":1}},"ecc8bf1944114aecbd7e324c8c2bf507":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7fe8fe9ab6546a7a6d1c04ef7cc3bf2","placeholder":"​","style":"IPY_MODEL_69fbd29bd0844b329bddb5802c9e816f","value":" 0/? [00:00&lt;?, ? tables/s]"}},"c4360706c2424953a8f3d30b4f2f4210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4bfce7e56cbe4b41989e65c5d0c246bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de298a7d016f4596a89942986a286e3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8bfe741382344678206ad2fb906d812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"683957c70e2549579951d57fc7f8bba7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7fe8fe9ab6546a7a6d1c04ef7cc3bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69fbd29bd0844b329bddb5802c9e816f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98732fadce5c46c1b546c22e0c08c8d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dcdd1b2c1914d47a0b51d899da6feb2","IPY_MODEL_337527568d314bf585d1bfb4d3e277ac","IPY_MODEL_2594b4ac338f474eb369720b438caf02"],"layout":"IPY_MODEL_e36c19a64e5d4dee9475c21a39ec7e5d"}},"7dcdd1b2c1914d47a0b51d899da6feb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7de9e3566bc948888b7913fda88aa0cd","placeholder":"​","style":"IPY_MODEL_cb259ea24ad44bd6ad9274e2df0dd697","value":""}},"337527568d314bf585d1bfb4d3e277ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f8545359696450ab8af9ba9ab581024","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d990adf79694e03a1cf61dd6203b0d5","value":1}},"2594b4ac338f474eb369720b438caf02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea725a8b182e49729205866ff11c1847","placeholder":"​","style":"IPY_MODEL_a9985fccb11e4821bcc806dfe12d1ee8","value":" 0/? [00:00&lt;?, ? tables/s]"}},"e36c19a64e5d4dee9475c21a39ec7e5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7de9e3566bc948888b7913fda88aa0cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb259ea24ad44bd6ad9274e2df0dd697":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f8545359696450ab8af9ba9ab581024":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6d990adf79694e03a1cf61dd6203b0d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea725a8b182e49729205866ff11c1847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9985fccb11e4821bcc806dfe12d1ee8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e5a66b0fe6b4ecd91304db04758e7ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9c6bdeab9984e15aa7dcc01d0c958a4","IPY_MODEL_d44c464b6c524f138f2178fb50b63b3a","IPY_MODEL_783eac2fc8c84d6f8e5f940938838821"],"layout":"IPY_MODEL_5ea8fe61718a44e7905205e913ea7b32"}},"c9c6bdeab9984e15aa7dcc01d0c958a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3ed916243ec4e69bd4e88e15819d9be","placeholder":"​","style":"IPY_MODEL_ed8185597e8341d1a425d6b873d6d31d","value":"100%"}},"d44c464b6c524f138f2178fb50b63b3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df012618612545bc889cb5f5219cd9b0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af10654bc0ae46c2adb42379c76704a5","value":2}},"783eac2fc8c84d6f8e5f940938838821":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f10f3d65c17a4f99acfd2fde878aa587","placeholder":"​","style":"IPY_MODEL_2fd0c81cea5a4e4993d6a0768db1702d","value":" 2/2 [00:00&lt;00:00, 98.04it/s]"}},"5ea8fe61718a44e7905205e913ea7b32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3ed916243ec4e69bd4e88e15819d9be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed8185597e8341d1a425d6b873d6d31d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df012618612545bc889cb5f5219cd9b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af10654bc0ae46c2adb42379c76704a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f10f3d65c17a4f99acfd2fde878aa587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fd0c81cea5a4e4993d6a0768db1702d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5af359f57b7e4e17b22df4b5150c7956":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97caa7ca30a244a792aa07508af13055","IPY_MODEL_9edb51098178405f993d9c9a909aa962","IPY_MODEL_2cb83798821740578c0fa40b197de17e"],"layout":"IPY_MODEL_ea85af05241b4e7eb81870322cdd6d73"}},"97caa7ca30a244a792aa07508af13055":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbc2eb3712204d05b35ec232e5619a01","placeholder":"​","style":"IPY_MODEL_7ed370b5acef481d8cbb9d5c97b7c917","value":"100%"}},"9edb51098178405f993d9c9a909aa962":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8d062199bde4b10a742f824b1fee42a","max":3270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73c0a807f9004250bb3d121da50827fe","value":3270}},"2cb83798821740578c0fa40b197de17e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_792d03adbf324f5189c8f9f53b74d2d8","placeholder":"​","style":"IPY_MODEL_0493e8015bf14aefb3d0849c60869c2d","value":" 3270/3270 [00:00&lt;00:00, 13927.59ex/s]"}},"ea85af05241b4e7eb81870322cdd6d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbc2eb3712204d05b35ec232e5619a01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ed370b5acef481d8cbb9d5c97b7c917":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8d062199bde4b10a742f824b1fee42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c0a807f9004250bb3d121da50827fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"792d03adbf324f5189c8f9f53b74d2d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0493e8015bf14aefb3d0849c60869c2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a752d80586ae4ea19fabd6cf9396a535":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5638110be76041f088ae104882670094","IPY_MODEL_c16a16da07454db19a757b6aae8c85df","IPY_MODEL_a6db3f71e92747b0b423aa696b47fd1d"],"layout":"IPY_MODEL_78e48251d73a45c286cdb5d45e3190bf"}},"5638110be76041f088ae104882670094":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48ead1e9cd38489aad3ea7fd606d5e44","placeholder":"​","style":"IPY_MODEL_ec0fb795924840918e00bd3ae49a87d4","value":"100%"}},"c16a16da07454db19a757b6aae8c85df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aea9dbd3c8849b18e9d470fff80b545","max":3270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_108e4c23437e4a8abfa08cf4de0c3df6","value":3270}},"a6db3f71e92747b0b423aa696b47fd1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89255c8e73214fa8a2c3cd0c73114b76","placeholder":"​","style":"IPY_MODEL_748186de38604c5bbe033cb62c930abc","value":" 3270/3270 [14:07&lt;00:00,  3.82ex/s]"}},"78e48251d73a45c286cdb5d45e3190bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48ead1e9cd38489aad3ea7fd606d5e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec0fb795924840918e00bd3ae49a87d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aea9dbd3c8849b18e9d470fff80b545":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"108e4c23437e4a8abfa08cf4de0c3df6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89255c8e73214fa8a2c3cd0c73114b76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748186de38604c5bbe033cb62c930abc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12aa776f8b054710863bb0bc89431cfd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d199cf8692fd41ef8b80ae860cc544d1","IPY_MODEL_0343b9e9dee24bf3a50d090e9db44c2e","IPY_MODEL_3d7c290b07314c27b75dfa5d70a093c0"],"layout":"IPY_MODEL_04d3f34792db4fa8a2595e4f62cb7501"}},"d199cf8692fd41ef8b80ae860cc544d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c58c9e1b33484261ac4a612d86d827b8","placeholder":"​","style":"IPY_MODEL_96a660e4b3324b589ce8a7f4acc11e38","value":"model.safetensors: 100%"}},"0343b9e9dee24bf3a50d090e9db44c2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3638e19f449d47609b92ddefe354d014","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_930163d3798c47b88185ebe2c85107ba","value":548105171}},"3d7c290b07314c27b75dfa5d70a093c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ceb28a358c041b38316ec5c5ac25d48","placeholder":"​","style":"IPY_MODEL_8837218af4bd464bb3a573cad86830c4","value":" 548M/548M [00:02&lt;00:00, 204MB/s]"}},"04d3f34792db4fa8a2595e4f62cb7501":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c58c9e1b33484261ac4a612d86d827b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a660e4b3324b589ce8a7f4acc11e38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3638e19f449d47609b92ddefe354d014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"930163d3798c47b88185ebe2c85107ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ceb28a358c041b38316ec5c5ac25d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8837218af4bd464bb3a573cad86830c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fe39b65b18748de833f8a6bbde95f5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_144de3bb77e940328f41262284dffc0a","IPY_MODEL_7bcb95734fe24aa99966107b6410ec36","IPY_MODEL_3dca42e95325463799d5b4410b01fc19"],"layout":"IPY_MODEL_43821b3b9ce84512bd198ff3490d6334"}},"144de3bb77e940328f41262284dffc0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60aa0b5991a0491ea652666517d8ba4d","placeholder":"​","style":"IPY_MODEL_209da89c0b1a49abab5db862d22e770c","value":"generation_config.json: 100%"}},"7bcb95734fe24aa99966107b6410ec36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a324247c41e460c9dd0e4f7c2271611","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cf33e6bf07649bdbc46e603be0d7c17","value":124}},"3dca42e95325463799d5b4410b01fc19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29d629dc77ec4e9fbba82339ea3359f5","placeholder":"​","style":"IPY_MODEL_80082b5ab25240a88e0525edf6cbea95","value":" 124/124 [00:00&lt;00:00, 11.0kB/s]"}},"43821b3b9ce84512bd198ff3490d6334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60aa0b5991a0491ea652666517d8ba4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"209da89c0b1a49abab5db862d22e770c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a324247c41e460c9dd0e4f7c2271611":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf33e6bf07649bdbc46e603be0d7c17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29d629dc77ec4e9fbba82339ea3359f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80082b5ab25240a88e0525edf6cbea95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"l-OmQD2TH8O9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3df946b-8e42-4def-8200-efbfeb0ce46b","executionInfo":{"status":"ok","timestamp":1732597699596,"user_tz":300,"elapsed":23026,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for adapter-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.5/441.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install gcsfs==2023.9.2 fsspec==2023.9.2 transformers peft accelerate adapter-transformers bitsandbytes\n","!pip -q install datasets==2.6 -U"]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsE3JofM0aog","outputId":"1a079977-76dc-4abe-f3a2-39f93e8aff4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) \n","Token is valid (permission: fineGrained).\n","The token `dandandives` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `dandandives`\n"]}]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUCMoCAwyIKf","outputId":"d8010f41-a509-41e0-9657-4901ba037b3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 25 01:15:13 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   46C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gu3PdWlyRA6","outputId":"7d134d7c-9a37-40f9-d2da-a631d601acc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 56.9 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"boolq\")"],"metadata":{"id":"zSzs5--7FQ5I","colab":{"base_uri":"https://localhost:8080/","height":390,"referenced_widgets":["c2cac5eb329f437fbfeb0f77d9dd0034","f68f90d6d2bb41a5b225d4b8b8874daf","7492cba8ee504271b9e4822cbddb20d3","7bac94fc39a44a7a87a1274bfb1e767e","87223e45e70c4a299385ca42c463bd57","374fa9b22fa647278c6d733cb91d3492","1d521ab9a2194b269cfaaed670d91b52","a9c8c82b77894de88b7a8ceefeca43b6","40b61569fe744cf184e6e5ea4b218427","9700b3913957485da141baac1e136547","daf52d70782c4fefa82a25904b208bb3","24d4d6304ceb488dac7f15cd2fac820a","a7c0c3ee652a4f4c93492783fbfc7b16","07929a20484745fe89fec46cbf9182a4","d26f7a5b39304e9e89eadde8635345f9","93101fb671ea4cdd9d04984976e2cf95","445a319833564a5a97c8002b3a383d0c","8cbcbd93970748ac83688ee18993af7b","8d460d0f0cad4c19adf462220c1e5c64","bfea3ee0d2d542c0b31fe99bd3daa5ad","3b6d3f46f6034e6496e99eb13cab3bae","cd5d55f4a5774b6c9ea3f7d33b757b27","e60754232204491bad128476a7e9074e","b2199d41160f49d0be41225d6c5ebbe7","0e1c266d4b634729be110966111a6249","330903ab409e43b2a4aaf27ba561cd05","150b7d3320164f96a02cd20a60180de7","c4fb1e9f65994aef892291963ab116ce","7d4c415b019b4bfaacf361d866a6267a","41ad99b08f594e9c8c9cb22368e1c272","97ffd4ce0b8c42fabbdf43cb09b04650","bb06ba2b1ab94e218843aaac20ec0e13","0a2306db7b5d44498ed2d247b4bad415","4a7a8e4782e349b8ad48f90c5dd0ee9b","fd28806be7f44834b41ae9a2dd25f94b","1e12861d69c044eab02eeeccbdbd4b3e","da597767e7d246cba45862beb0b4ae37","8ac9c5bb8fa24613879e323fd207d8d5","605880ef77a54504ad42cd74c9de26d8","53f89ec859a2402d9ea5e695eefd91e4","0d5a7b6125b143d1852bb748e7752a02","c8496c3dc441460fb8bc67e958ba4d63","61efdb12bac24032b1feeb23d6b3ab17","b14d7a1a82ee49f38ce2e114004de6b7","cffb7ed5b08f4a4aa50bfe878ee88e23","15362b965bfa4af8a3c470b3de8b5c4e","29cb460d11d64a8dbda8c67a75a5dae9","0ea9f64f201f4ac7b978b19fe1836a44","b939c7af30c14a2a918aba53f11bc1de","08b80efe5f044d6aaaa9edc3a4bdb366","5cec17d289a24f0f94d764d65107af62","610cba62284c4aeeb85a012fc94e25bf","ed04740e7ded484aba77bb4552ef5834","aeb904d9deb74586a7e65e64765edbf0","da16f11a6db64bee9357435122e697ed","1a488d8bad4c49babd73ef0a4bc94f99","2bc659f2cbe7498284bcc7cca5825e34","ef1839c460df4a2e972f3495e08328ff","ecc8bf1944114aecbd7e324c8c2bf507","c4360706c2424953a8f3d30b4f2f4210","4bfce7e56cbe4b41989e65c5d0c246bd","de298a7d016f4596a89942986a286e3d","e8bfe741382344678206ad2fb906d812","683957c70e2549579951d57fc7f8bba7","c7fe8fe9ab6546a7a6d1c04ef7cc3bf2","69fbd29bd0844b329bddb5802c9e816f","98732fadce5c46c1b546c22e0c08c8d0","7dcdd1b2c1914d47a0b51d899da6feb2","337527568d314bf585d1bfb4d3e277ac","2594b4ac338f474eb369720b438caf02","e36c19a64e5d4dee9475c21a39ec7e5d","7de9e3566bc948888b7913fda88aa0cd","cb259ea24ad44bd6ad9274e2df0dd697","8f8545359696450ab8af9ba9ab581024","6d990adf79694e03a1cf61dd6203b0d5","ea725a8b182e49729205866ff11c1847","a9985fccb11e4821bcc806dfe12d1ee8","4e5a66b0fe6b4ecd91304db04758e7ea","c9c6bdeab9984e15aa7dcc01d0c958a4","d44c464b6c524f138f2178fb50b63b3a","783eac2fc8c84d6f8e5f940938838821","5ea8fe61718a44e7905205e913ea7b32","e3ed916243ec4e69bd4e88e15819d9be","ed8185597e8341d1a425d6b873d6d31d","df012618612545bc889cb5f5219cd9b0","af10654bc0ae46c2adb42379c76704a5","f10f3d65c17a4f99acfd2fde878aa587","2fd0c81cea5a4e4993d6a0768db1702d"]},"outputId":"e9d0bf51-681c-4cf6-f175-d7c7af390a70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2cac5eb329f437fbfeb0f77d9dd0034"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration boolq-150dc58bba7bf36d\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d4d6304ceb488dac7f15cd2fac820a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/3.69M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e60754232204491bad128476a7e9074e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.26M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7a8e4782e349b8ad48f90c5dd0ee9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffb7ed5b08f4a4aa50bfe878ee88e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 tables [00:00, ? tables/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a488d8bad4c49babd73ef0a4bc94f99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 tables [00:00, ? tables/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98732fadce5c46c1b546c22e0c08c8d0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5a66b0fe6b4ecd91304db04758e7ea"}},"metadata":{}}]},{"cell_type":"code","source":["import importlib\n","import math\n","import re\n","import warnings\n","from dataclasses import asdict, dataclass, field\n","import enum\n","import json\n","import os\n","import sys\n","from enum import Enum\n","from typing import List, Optional, Union\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from huggingface_hub import hf_hub_download\n","from transformers.activations import ACT2FN\n","from transformers.utils import PushToHubMixin"],"metadata":{"id":"CydE2LS7GmUh","executionInfo":{"status":"ok","timestamp":1732597761608,"user_tz":300,"elapsed":4230,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# in LLM-Adapters/peft/src/peft/utils/config.py\n","\n","class PeftType(str, enum.Enum):\n","    PROMPT_TUNING = \"PROMPT_TUNING\"\n","    P_TUNING = \"P_TUNING\"\n","    PREFIX_TUNING = \"PREFIX_TUNING\"\n","    LORA = \"LORA\"\n","    BOTTLENECK = \"BOTTLENECK\"\n","\n","class TaskType(str, enum.Enum):\n","    SEQ_CLS = \"SEQ_CLS\"\n","    SEQ_2_SEQ_LM = \"SEQ_2_SEQ_LM\"\n","    CAUSAL_LM = \"CAUSAL_LM\"\n","    TOKEN_CLS = \"TOKEN_CLS\"\n","\n","@dataclass\n","class PeftConfigMixin(PushToHubMixin):\n","    r\"\"\"\n","    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\n","    PEFT adapter models. This class inherits from `transformers.utils.PushToHubMixin` which contains the methods to\n","    push your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\n","    directory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n","\n","    Args:\n","        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n","    \"\"\"\n","    peft_type: Optional[PeftType] = field(default=None, metadata={\"help\": \"The type of PEFT model.\"})\n","\n","    @property\n","    def __dict__(self):\n","        return asdict(self)\n","\n","    def to_dict(self):\n","        return self.__dict__\n","\n","    def save_pretrained(self, save_directory, **kwargs):\n","        r\"\"\"\n","        This method saves the configuration of your adapter model in a directory.\n","\n","        Args:\n","            save_directory (`str`):\n","                The directory where the configuration will be saved.\n","            **kwargs:\n","                Additional keyword arguments passed along to the `transformers.utils.PushToHubMixin.push_to_hub`\n","                method.\n","        \"\"\"\n","        if os.path.isfile(save_directory):\n","            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n","\n","        os.makedirs(save_directory, exist_ok=True)\n","\n","        output_dict = self.__dict__\n","        output_path = os.path.join(save_directory, CONFIG_NAME)\n","\n","        # save it\n","        with open(output_path, \"w\") as writer:\n","            writer.write(json.dumps(output_dict, indent=2, sort_keys=True))\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n","        r\"\"\"\n","        This method loads the configuration of your adapter model from a directory.\n","\n","        Args:\n","            pretrained_model_name_or_path (`str`):\n","                The directory or the hub-id where the configuration is saved.\n","            **kwargs:\n","                Additional keyword arguments passed along to the child class initialization.\n","        \"\"\"\n","        if os.path.isfile(os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):\n","            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n","        else:\n","            try:\n","                config_file = hf_hub_download(pretrained_model_name_or_path, CONFIG_NAME)\n","            except Exception:\n","                raise ValueError(f\"Can't find config.json at '{pretrained_model_name_or_path}'\")\n","\n","        loaded_attributes = cls.from_json_file(config_file)\n","\n","        config = cls(**kwargs)\n","\n","        for key, value in loaded_attributes.items():\n","            if hasattr(config, key):\n","                setattr(config, key, value)\n","\n","        return config\n","\n","    @classmethod\n","    def from_json_file(cls, path_json_file, **kwargs):\n","        r\"\"\"\n","        Loads a configuration file from a json file.\n","\n","        Args:\n","            path_json_file (`str`):\n","                The path to the json file.\n","        \"\"\"\n","        with open(path_json_file, \"r\") as file:\n","            json_object = json.load(file)\n","\n","        return json_object\n","\n","\n","@dataclass\n","class PeftConfig(PeftConfigMixin):\n","    \"\"\"\n","    This is the base configuration class to store the configuration of a :class:`~peft.PeftModel`.\n","\n","    Args:\n","        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n","        task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n","        inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n","    \"\"\"\n","\n","    base_model_name_or_path: str = field(default=None, metadata={\"help\": \"The name of the base model to use.\"})\n","    peft_type: Union[str, PeftType] = field(default=None, metadata={\"help\": \"Peft type\"})\n","    task_type: Union[str, TaskType] = field(default=None, metadata={\"help\": \"Task type\"})\n","    inference_mode: bool = field(default=False, metadata={\"help\": \"Whether to use inference mode\"})"],"metadata":{"id":"DOvRcrNLGoJP","executionInfo":{"status":"ok","timestamp":1732597761608,"user_tz":300,"elapsed":3,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# in LLM-Adapters/peft/src/peft/utils/other.py\n","from transformers import Conv1D\n","\n","\n","def transpose(weight, fan_in_fan_out):\n","    return weight.T if fan_in_fan_out else weight\n","\n","\n","def is_bnb_available():\n","    return importlib.util.find_spec(\"bitsandbytes\") is not None\n","\n","\n","if is_bnb_available():\n","    import bitsandbytes as bnb\n","\n","\n","\n","class AdapterLayer:\n","    def __init__(\n","        self,\n","        bottleneck_size: int,\n","        non_linearity: str,\n","        adapter_dropout: float,\n","        scaling: Union[float, str],\n","    ):\n","        self.bottleneck_size = bottleneck_size\n","        self.non_linearity = non_linearity\n","        self.scaling = scaling\n","        #optional dropout\n","        if adapter_dropout > 0.0:\n","            self.adapter_dropout = nn.Dropout(p=adapter_dropout)\n","        else:\n","            self.adapter_dropout = lambda x: x\n","        self.disable_adapters = False\n","\n","\n","class Linear(nn.Linear, AdapterLayer):\n","    \"\"\"\n","    Bottleneck adapter in a dense layer. The adapter can be applied after the multi-head attention layer and/or\n","    after the feed-forward layer.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        in_features: int,\n","        out_features: int,\n","        adapter_type: str,\n","        bottleneck_size: int,\n","        non_linearity: str,\n","        adapter_dropout: float,\n","        scaling: Union[float, str],\n","        init_weights: str,\n","        **kwargs,\n","    ):\n","        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n","        AdapterLayer.__init__(self, bottleneck_size=bottleneck_size,\n","                                non_linearity=non_linearity,\n","                                adapter_dropout=adapter_dropout,\n","                                scaling=scaling)\n","\n","        self.init_weights = init_weights\n","        self.adapter_type = adapter_type\n","        if isinstance(scaling, float):\n","            self.adapter_scaling = scaling\n","        elif scaling == \"learned\":\n","            self.adapter_scaling = nn.Parameter(torch.ones(1))\n","        # Actual trainable parameters\n","        self.adapter_down = nn.Linear(in_features, bottleneck_size, bias=False)\n","        self.adapter_up = nn.Linear(bottleneck_size, out_features, bias=False)\n","        self.act_fn = ACT2FN[self.non_linearity]\n","        #Freezing the pre-trained weight matrix\n","        self.weight.requires_grad = False\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.Linear.reset_parameters(self)\n","        # if we want to initialize with the bert strategy then this function is called for all the linear layers\n","        if hasattr(self, \"adapter_down\"):\n","            if self.init_weights == \"bert\":\n","                self.adapter_down.apply(self.init_bert_weights)\n","                self.adapter_up.apply(self.init_bert_weights)\n","            elif self.init_weights == \"mam_adapter\":\n","                nn.init.kaiming_uniform_(self.adapter_down.weight, a=math.sqrt(5))\n","                nn.init.zeros_(self.adapter_up.weight)\n","            else:\n","                raise ValueError(\"Unknown init_weights type: {}\".format(config[\"init_weights\"]))\n","\n","    # This is copied from the BertPreTrainedModel class to make this a self containing class.\n","    @staticmethod\n","    def init_bert_weights(module):\n","        \"\"\"Initialize the weights.\"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # std defaults to 0.02, this might need to be changed\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def train(self, mode: bool = True):\n","        nn.Linear.train(self, mode)\n","        self.adapter_down.train(mode)\n","        self.adapter_up.train(mode)\n","\n","    def eval(self):\n","        nn.Linear.eval(self)\n","        self.adapter_down.eval()\n","        self.adapter_up.eval()\n","\n","    def forward(self, x: torch.Tensor):\n","        if self.disable_adapters:\n","            return F.linear(x, self.weight, bias=self.bias)\n","        else:\n","            if self.adapter_type == \"mh_adapter\":\n","                # for mh_adapter, x will pass the adapter first and then the linear layer\n","                expected_dtype = x.dtype\n","                residual = x\n","\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","                output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))).to(expected_dtype) * self.adapter_scaling\n","\n","                output = output + residual\n","\n","                result = F.linear(output, self.weight, bias=self.bias)\n","            elif self.adapter_type == \"output_adapter\":\n","                # for output_adapter, x will pass the linear layer first and then the adapter\n","                x = F.linear(x, self.weight, bias=self.bias)\n","                expected_dtype = x.dtype\n","                residual = x\n","\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","\n","                output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))).to(expected_dtype) * self.adapter_scaling\n","\n","                result = output + residual\n","            elif self.adapter_type == \"parallel_adapter\":\n","                # for parallel_adapter, x will pass the linear layer first and the adapter layer parallelly.\n","                # The output of the adapter layer will be added to the output of the linear layer\n","                result = F.linear(x, self.weight, bias=self.bias)\n","                expected_dtype = result.dtype\n","\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","                output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))).to(expected_dtype) * self.adapter_scaling\n","\n","                result = result + output\n","            return result\n","\n","\n","if is_bnb_available():\n","\n","    class Linear8bitLt(bnb.nn.Linear8bitLt, AdapterLayer):\n","        # Aadapter layer for 8bit linear layer\n","        def __init__(\n","            self,\n","            in_features: int,\n","            out_features: int,\n","            adapter_type: str,\n","            bottleneck_size: int,\n","            non_linearity: str,\n","            adapter_dropout: float,\n","            scaling: Union[float, str],\n","            init_weights: str,\n","            **kwargs,\n","        ):\n","            bnb.nn.Linear8bitLt.__init__(\n","                self,\n","                in_features,\n","                out_features,\n","                bias=kwargs.get(\"bias\", True),\n","                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n","                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n","                threshold=kwargs.get(\"threshold\", 0.0),\n","                index=kwargs.get(\"index\", None),\n","            )\n","            AdapterLayer.__init__(\n","                self,\n","                bottleneck_size=bottleneck_size,\n","                non_linearity=non_linearity,\n","                adapter_dropout=adapter_dropout,\n","                scaling=scaling,)\n","\n","            self.init_weights = init_weights\n","            self.adapter_type = adapter_type\n","            if isinstance(scaling, float):\n","                self.adapter_scaling = scaling\n","            elif scaling == \"learned\":\n","                self.adapter_scaling = nn.Parameter(torch.ones(1))\n","            # Actual trainable parameters\n","            self.adapter_down = nn.Linear(in_features, bottleneck_size, bias=False)\n","            self.adapter_up = nn.Linear(bottleneck_size, out_features, bias=False)\n","            self.act_fn = ACT2FN[self.non_linearity]\n","            #Freezing the pre-trained weight matrix\n","            self.weight.requires_grad = False\n","            self.reset_parameters()\n","\n","        def reset_parameters(self):\n","            nn.Linear.reset_parameters(self)\n","            # if we want to initialize with the bert strategy then this function is called for all the linear layers\n","            if hasattr(self, \"adapter_down\"):\n","                if self.init_weights == \"bert\":\n","                    self.adapter_down.apply(self.init_bert_weights)\n","                    self.adapter_up.apply(self.init_bert_weights)\n","                elif self.init_weights == \"mam_adapter\":\n","                    nn.init.kaiming_uniform_(self.adapter_down.weight, a=math.sqrt(5))\n","                    nn.init.zeros_(self.adapter_up.weight)\n","                else:\n","                    raise ValueError(\"Unknown init_weights type: {}\".format(config[\"init_weights\"]))\n","\n","        # This is copied from the BertPreTrainedModel class to make this a self containing class.\n","        @staticmethod\n","        def init_bert_weights(module):\n","            \"\"\"Initialize the weights.\"\"\"\n","            if isinstance(module, (nn.Linear, nn.Embedding)):\n","                # std defaults to 0.02, this might need to be changed\n","                module.weight.data.normal_(mean=0.0, std=0.02)\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                module.bias.data.zero_()\n","\n","        def forward(self, x: torch.Tensor):\n","            result_pre_forward = super().forward(x)\n","\n","            if self.disable_adapters:\n","                return result_pre_forward\n","            else:\n","                if self.adapter_type == \"mh_adapter\":\n","                    if not torch.is_autocast_enabled():\n","                        expected_dtype = x.dtype\n","\n","                        if x.dtype != torch.float32:\n","                            x = x.float()\n","\n","                        residual = x\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))).to(expected_dtype) * self.adapter_scaling\n","                        output = (output + residual).to(expected_dtype)\n","\n","                        result = super().forward(output)\n","                    else:\n","                        residual = x\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))) * self.adapter_scaling\n","                        output = output + residual\n","\n","                        print(output.shape, \"2\")\n","\n","                        result = super().forward(output)\n","                elif self.adapter_type == \"output_adapter\":\n","                    if not torch.is_autocast_enabled():\n","                        expected_dtype = result_pre_forward.dtype\n","\n","                        if result_pre_forward.dtype != torch.float32:\n","                            result_pre_forward = result_pre_forward.float()\n","\n","                        residual = result_pre_forward\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(result_pre_forward)))).to(expected_dtype) * self.adapter_scaling\n","                        result = (output + residual).to(expected_dtype)\n","                    else:\n","                        residual = result_pre_forward\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(result_pre_forward)))) * self.adapter_scaling\n","                        result = output + residual\n","                elif self.adapter_type == \"parallel_adapter\":\n","                    if not torch.is_autocast_enabled():\n","                        expected_dtype = result_pre_forward.dtype\n","\n","                        if x.dtype != torch.float32:\n","                            x = x.float()\n","\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))).to(expected_dtype) * self.adapter_scaling\n","                        result = result_pre_forward + output\n","                    else:\n","                        output = self.adapter_up(self.act_fn(self.adapter_down(self.adapter_dropout(x)))) * self.adapter_scaling\n","                        result = result_pre_forward + output\n","\n","                return result\n","\n","\n","class Conv1DAdapter(Conv1D, AdapterLayer):\n","    def __init__(\n","        self,\n","        nf: int,\n","        nx: int,\n","        adapter_type: str,\n","        bottleneck_size: int,\n","        non_linearity: str,\n","        adapter_dropout: float,\n","        scaling: Union[float, str],\n","        init_weights: str,\n","        **kwargs,\n","    ):\n","        Conv1D.__init__(self, nf, nx)\n","        AdapterLayer.__init__(\n","            self,\n","            bottleneck_size=bottleneck_size,\n","            non_linearity=non_linearity,\n","            adapter_dropout=adapter_dropout,\n","            scaling=scaling,\n","        )\n","\n","        self.nx = nx\n","        self.nf = nf\n","        self.init_weights = init_weights\n","        self.adapter_type = adapter_type\n","        if isinstance(scaling, float):\n","            self.adapter_scaling = scaling\n","        elif scaling == \"learned\":\n","            self.adapter_scaling = nn.Parameter(torch.ones(1))\n","        else:\n","            self.adapter_scaling = 1.0  # Default scaling\n","        # Adapter layers\n","        self.adapter_down = Conv1D(bottleneck_size, nx)\n","        self.adapter_up = Conv1D(nf, bottleneck_size)\n","        self.act_fn = ACT2FN[self.non_linearity]\n","        # Freeze the pre-trained weights\n","        self.weight.requires_grad = False\n","        self.bias.requires_grad = False\n","    def train(self, mode: bool = True):\n","        torch.nn.Conv1d.train(self, mode)\n","        self.adapter_down.train(mode)\n","        self.adapter_up.train(mode)\n","    def eval(self):\n","        torch.nn.Conv1d.eval(self)\n","        self.adapter_down.eval()\n","        self.adapter_up.eval()\n","    def forward(self, x: torch.Tensor):\n","        if self.disable_adapters:\n","            return super().forward(x)\n","        else:\n","            if self.adapter_type == \"mh_adapter\":\n","                # For mh_adapter, x will pass the adapter first and then the convolutional layer\n","                expected_dtype = x.dtype\n","                residual = x\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","                output = (\n","                    self.adapter_up(\n","                        self.act_fn(self.adapter_down(self.adapter_dropout(x)))\n","                    ).to(expected_dtype)\n","                    * self.adapter_scaling\n","                )\n","                output = output + residual\n","\n","                # print(output.size()[:-1] + (self.nf,))\n","                # print(output.shape, residual.shape, \"1\")\n","                # x = x.view(*size_out)\n","                result = F.linear(output, self.weight.T, bias=self.bias)\n","\n","                # print(output.shape, result.shape)\n","                # result = super().forward(output)\n","                # result = output + residual\n","            elif self.adapter_type == \"output_adapter\":\n","                # For output_adapter, x will pass the convolutional layer first and then the adapter\n","                # x = super().forward(x)\n","                x = F.linear(x, self.weight.T, bias=self.bias)\n","                expected_dtype = x.dtype\n","                residual = x\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","                output = (\n","                    self.adapter_up(\n","                        self.act_fn(self.adapter_down(self.adapter_dropout(x)))\n","                    ).to(expected_dtype)\n","                    * self.adapter_scaling\n","                )\n","                result = output + residual\n","            elif self.adapter_type == \"parallel_adapter\":\n","                # For parallel_adapter, x will pass the convolutional layer first and the adapter layer parallelly.\n","                # The output of the adapter layer will be added to the output of the convolutional layer\n","                result = super().forward(x)\n","                expected_dtype = result.dtype\n","                if x.dtype != torch.float32:\n","                    x = x.float()\n","                output = (\n","                    self.adapter_up(\n","                        self.act_fn(self.adapter_down(self.adapter_dropout(x)))\n","                    ).to(expected_dtype)\n","                    * self.adapter_scaling\n","                )\n","                result = result + output\n","            else:\n","                raise ValueError(\n","                    f\"Unknown adapter_type: {self.adapter_type}. Expected one of ['mh_adapter', 'output_adapter', 'parallel_adapter'].\"\n","                )\n","            return result"],"metadata":{"id":"8OIrVlYDHmcI","executionInfo":{"status":"ok","timestamp":1732597767523,"user_tz":300,"elapsed":1301,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["TRANSFORMERS_MODELS_TO_ADAPTER_TYPE_MAPPING = {\n","    \"bloom\": {\"dense_h_to_4h\": \"mh_adapter\", \"dense_4h_to_h\": \"output_adapter\"},\n","    \"gptj\": {\"fc_in\":\"mh_adapter\", \"fc_out\":\"output_adapter\"},\n","    \"gpt_neo\": {\"c_fc\":\"mh_adapter\", \"c_proj\":\"output_adapter\"},\n","    \"gpt2\": {\"c_fc\":\"mh_adapter\", \"c_proj\":\"output_adapter\"},\n","    \"llama\": {\"gate_proj\": \"mh_adapter\", \"up_proj\":\"mh_adapter\", \"down_proj\":\"output_adapter\"},\n","    \"opt\": {\"fc1\":\"mh_adapter\", \"fc2\":\"output_adapter\"},\n","    \"chatglm\": {\"dense_h_to_4h\": \"mh_adapter\", \"dense_4h_to_h\": \"output_adapter\"},\n","}\n","\n","@dataclass\n","class BottleneckConfig(PeftConfig):\n","    \"\"\"\n","    This is the configuration class to store the configuration of a [`~peft.Bottleneck`].\n","\n","    Args:\n","        bottleneck_size (`int`): The size of the bottleneck.\n","        non_linearity (`str`): The non-linearity to apply to the bottleneck.\n","        dropout (`float`, optional): The dropout probability of the bottleneck. Default to 0.0\n","        bias ('str'): Bias type for Bottleneck. Can be 'none', 'all' or 'adapter_only'. Default to 'none'.\n","        use_parallel_adapter (:obj:`bool`, optional): Whether to use parallel adapter. Defaults to False.\n","        scaling (:obj:`float` or :obj:`str`, optional):\n","            Scaling factor to use for scaled addition of adapter outputs as done by He et al. (2021). Can be either a\n","            constant factor (float) or the string \"learned\", in which case the scaling factor is learned. Defaults to\n","            1.0.\n","        target_modules (`Union[List[str],str]`): The names of the modules to apply Adapter to.\n","        init_weights (:obj:`str`, optional): Initialization method for the weights of the adapter modules.\n","            Currently, this can be either \"bert\" (default) or \"mam_adapter\".\n","        modules_to_save (`List[str]`):List of modules apart from Bottleneck adapter layers to be set as trainable\n","            and saved in the final checkpoint.\n","    \"\"\"\n","    bottleneck_size : int = field(default=256, metadata={\"help\": \"The size of the bottleneck\"})\n","    non_linearity : str = field(default=\"tanh\", metadata={\"help\": \"The non-linearity to apply to the bottleneck\"})\n","    adapter_dropout : float = field(default=0.0, metadata={\"help\": \"The dropout probability of the bottleneck, default to 0.0\"})\n","    target_modules: Optional[Union[List[str], str]] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"List of module names or regex expression of the module names to replace with Adapter.\"\n","            \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' \"\n","        },\n","    )\n","    use_parallel_adapter: bool = field(default=False, metadata={\"help\": \"Whether to use parallel adapter\"})\n","    use_adapterp: bool = field(default=False, metadata={\"help\": \"Whether to use adapterp\"})\n","    scaling: Union[float, str] = 1.0\n","    bias: str = field(default=\"none\", metadata={\"help\": \"Bias type for Bottleneck. Can be 'none', 'all' or 'adapter_only'\"})\n","    init_weights: str = field(default=\"bert\", metadata={\"help\": \"Initialization method for the weights of the adapter modules.\"})\n","    modules_to_save: Optional[List[str]] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"List of modules apart from Adapter layers to be set as trainable and saved in the final checkpoint. \"\n","            \"For example, in Sequence Classification or Token Classification tasks, \"\n","            \"the final layer `classifier/score` are randomly initialized and as such need to be trainable and saved.\"\n","        },\n","    )\n","\n","    def __post_init__(self):\n","        self.peft_type = PeftType.BOTTLENECK\n","\n","class BottleneckModel(torch.nn.Module):\n","    \"\"\"\n","    Creates Bottleneck adapter model for a pretrained trainsformers model.\n","\n","    Args:\n","        model ('transformers.PreTrainedModel'): The pretrained model to be adapted.\n","        config (`BottleneckConfig`): The configuration of the Bottleneck adapter.\n","\n","    Returns:\n","        `torch.nn.Module`: The Bottleneck adapter model.\n","\n","    Example::\n","\n","        >>> from transformers import AutoModelForCausalLM, BottleneckConfig\n","        >>> from peft import BottleneckModel, BottleneckConfig\n","        >>> config = BottleneckConfig(\n","            peft_type=\"BOTTLNECK\", task=\"CAUSAL_LM\", target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"],\n","            bottleneck_size=256, non_linearity=\"tanh\",\n","        )\n","        >>> model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n","        >>> bottleneck_model = BottleneckModel(config, model)\n","\n","    **Attribute**:\n","        - **model** (`transformers.PreTrainedModel`): The pretrained model to be adapted.\n","        - **peft_config** (`BottleneckConfig`): The configuration of the Bottleneck adapter.\n","    \"\"\"\n","\n","    def __init__(self, config, model):\n","        super().__init__()\n","        self.model = model\n","        self.peft_config = config\n","        self._find_and_replace()\n","        mark_only_adapter_as_trainable(self.model, self.peft_config.bias)\n","        self.forward = self.model.forward\n","        print(self.forward)\n","\n","    def _find_and_replace(self):\n","        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n","        if loaded_in_8bit and not is_bnb_available():\n","            raise ImportError(\n","                \"To use Adapter with 8-bit quantization, please install the `bitsandbytes` package. \"\n","                \"You can install it with `pip install bitsandbytes`.\"\n","            )\n","        is_target_modules_in_base_model = False\n","        is_hf_device_map_available = hasattr(self.model, \"hf_device_map\")\n","        kwargs = {\n","            \"bottleneck_size\": self.peft_config.bottleneck_size,\n","            \"non_linearity\": self.peft_config.non_linearity,\n","            \"adapter_dropout\": self.peft_config.adapter_dropout,\n","            \"scaling\": self.peft_config.scaling,\n","            \"init_weights\": self.peft_config.init_weights,\n","        }\n","        key_list = [key for key, _ in self.model.named_modules()]\n","        for key in key_list:\n","            if isinstance(self.peft_config.target_modules, str):\n","                target_module_found = re.fullmatch(self.peft_config.target_modules, key)\n","            else:\n","                target_module_found = any(key.endswith(target_key) for target_key in self.peft_config.target_modules)\n","            if target_module_found:\n","                if not is_target_modules_in_base_model:\n","                    is_target_modules_in_base_model = True\n","                parent, target, target_name = self._get_submodules(key)\n","                # determine the type of adapter to be used, this will effect the forward pass\n","                if self.peft_config.use_parallel_adapter:\n","                    adapter_type = \"parallel_adapter\"\n","                else:\n","                    adapter_type = TRANSFORMERS_MODELS_TO_ADAPTER_TYPE_MAPPING[self.model.config.model_type][target_name]\n","                kwargs.update({\"adapter_type\": adapter_type})\n","\n","                bias = target.bias is not None\n","                # print(loaded_in_8bit, isinstance(target, bnb.nn.Linear8bitLt), target, isinstance(target, Conv1D))\n","                if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n","                    kwargs.update(\n","                        {\n","                            \"has_fp16_weights\": target.state.has_fp16_weights,\n","                            \"memory_efficient_backward\": target.state.memory_efficient_backward,\n","                            \"threshold\": target.state.threshold,\n","                            \"index\": target.index,\n","                        }\n","                    )\n","                    if adapter_type == \"mh_adapter\":\n","                        new_module = Linear8bitLt(target.in_features, target.in_features, bias=bias, **kwargs)\n","                    elif adapter_type == \"output_adapter\":\n","                        new_module = Linear8bitLt(target.out_features, target.out_features, bias=bias, **kwargs)\n","                    elif adapter_type == \"parallel_adapter\":\n","                        new_module = Linear8bitLt(target.in_features, target.out_features, bias=bias, **kwargs)\n","                elif isinstance(target, torch.nn.Linear):\n","                    if adapter_type == \"mh_adapter\":\n","                        new_module = Linear(target.in_features, target.in_features, bias=bias, **kwargs)\n","                    elif adapter_type == \"output_adapter\":\n","                        new_module = Linear(target.out_features, target.out_features, bias=bias, **kwargs)\n","                    elif adapter_type == \"parallel_adapter\":\n","                        new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)\n","                # elif isinstance(target, Conv1D):\n","                #     if adapter_type == \"mh_adapter\":\n","                #         new_module = Linear(target.nx, target.nx, bias=bias, **kwargs)\n","                #     elif adapter_type == \"output_adapter\":\n","                #         new_module = Linear(target.nf, target.nf, bias=bias, **kwargs)\n","                #     elif adapter_type == \"parallel_adapter\":\n","                #         new_module = Linear(target.nx, target.nf, bias=bias, **kwargs)\n","                elif isinstance(target, Conv1D):\n","                    if adapter_type == \"mh_adapter\":\n","                        nx = target.nx  # Input features\n","                        nf = target.nx  # Output features\n","                    elif adapter_type == \"output_adapter\":\n","                        nx = target.nf               # Input features\n","                        nf = target.nf               # Output features\n","                    elif adapter_type == \"parallel_adapter\":\n","                        nx = target.nx  # Input features\n","                        nf = target.nf               # Output features\n","                    new_module = Conv1DAdapter(\n","                      nf=nf,\n","                      nx=nx,\n","                      **kwargs\n","                    )\n","                self._replace_module(parent, target_name, new_module, target)\n","        if not is_target_modules_in_base_model:\n","            raise ValueError(\n","                f\"Target modules {self.peft_config.target_modules} not found in the base model. \"\n","                f\"Please check the target modules and try again.\"\n","            )\n","\n","    def _get_submodules(self, key):\n","        parent = self.model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n","        target_name = key.split(\".\")[-1]\n","        target = self.model.get_submodule(key)\n","        return parent, target, target_name\n","\n","    def _replace_module(self, parent_module, child_name, new_module, old_module):\n","        setattr(parent_module, child_name, new_module)\n","        new_module.weight = old_module.weight\n","        if old_module.bias is not None:\n","            new_module.bias = old_module.bias\n","        if getattr(old_module, \"state\", None) is not None:\n","            new_module.state = old_module.state\n","            new_module.to(old_module.weight.device)\n","\n","        # dispatch to correct device\n","        for name, module in new_module.named_modules():\n","            if \"adapter_\" in name:\n","                module.to(old_module.weight.device)\n","\n","    def __getattr__(self, name: str):\n","        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n","        try:\n","            return super().__getattr__(name)  # defer to nn.Module's logic\n","        except AttributeError:\n","            return getattr(self.model, name)\n","\n","    @property\n","    def modules_to_save(self):\n","        return None\n","\n","    def get_peft_config_as_dict(self, inference: bool = False):\n","        config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(self.peft_config).items()}\n","        if inference:\n","            config[\"inference_mode\"] = True\n","        return config\n","\n","    def _set_adapter_layers(self, enabled=True):\n","        for module in self.model.modules():\n","            if isinstance(module, AdapterLayer):\n","                module.disable_adapters = False if enabled else True\n","\n","    def enable_adapter_layers(self):\n","        self._set_adapter_layers(enabled=True)\n","\n","    def disable_adapter_layers(self):\n","        self._set_adapter_layers(enabled=False)\n","\n","\n","# Below code is based on https://github.com/adapter-hub/adapter-transformers/blob/master/src/transformers/adapters/modeling.py and lora.py from huggingfance PEFT\n","# and modified to work with PyTorch FSDP\n","\n","\n","#  ------------------------------------------------------------------------------------------\n","#  Copyright (c) Microsoft Corporation. All rights reserved.\n","#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n","#  ------------------------------------------------------------------------------------------\n","\n","\n","# Copy from lora.py\n","# had to adapt it for `lora_only` to work\n","def mark_only_adapter_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n","    for n, p in model.named_parameters():\n","        if \"adapter_\" not in n:\n","            p.requires_grad = False\n","    if bias == \"none\":\n","        return\n","    elif bias == \"all\":\n","        for n, p in model.named_parameters():\n","            if \"bias\" in n:\n","                p.requires_grad = True\n","    elif bias == \"adapter_only\":\n","        for m in model.modules():\n","            if isinstance(m, AdapterLayer) and hasattr(m, \"bias\") and m.bias is not None:\n","                m.bias.requires_grad = True\n","    else:\n","        raise NotImplementedError\n"],"metadata":{"id":"ALBVnR4yH6SS","executionInfo":{"status":"ok","timestamp":1732597776681,"user_tz":300,"elapsed":979,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XYf6VGsyH6Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X3-3XltFH6AB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_example(example):\n","    question = example['question']\n","    passage = example['passage']\n","    answer = 'Yes' if example['answer'] else 'No'\n","    prompt = f\"Question: {question}\\nPassage: {passage}\\nAnswer:\"\n","    return {'input_text': prompt, 'target_text': answer}\n","formatted_dataset = dataset.map(format_example, remove_columns=dataset['train'].column_names)"],"metadata":{"id":"6OU6RKDrFeEb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3738c39f-8ebf-45d1-ffa0-b648bda73bcf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4f39c03b8ffa0a61.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4e8fbb334b6284fb.arrow\n"]}]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM, AutoTokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n","# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n","\n","# tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def tokenize_function(examples):\n","    inputs = [f\"{input_text} {target_text}\" for input_text, target_text in zip(examples['input_text'], examples['target_text'])]\n","\n","    # Tokenize the combined text and return PyTorch tensors\n","    model_inputs = tokenizer(\n","        inputs,\n","        max_length=512,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt',  # Ensure tensors are returned\n","    )\n","\n","    # Create labels by cloning the input IDs\n","    labels = model_inputs['input_ids'].clone()\n","\n","    # Determine the length of the input_text for each example\n","    input_lengths = []\n","    for input_text in examples['input_text']:\n","        input_ids = tokenizer(\n","            input_text,\n","            max_length=512,\n","            truncation=True,\n","            add_special_tokens=False,  # Do not add special tokens here\n","        )['input_ids']\n","        input_lengths.append(len(input_ids))\n","\n","    # Mask the labels for the input_text portion\n","    for i, input_length in enumerate(input_lengths):\n","        labels[i, :input_length] = -100  # Use -100 to ignore the input tokens\n","\n","    model_inputs['labels'] = labels\n","\n","    return model_inputs\n","\n","tokenized_dataset = formatted_dataset.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=formatted_dataset['train'].column_names,\n",")"],"metadata":{"id":"5N7y9nSaFr5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,  # We are not using masked language modeling\n",")"],"metadata":{"id":"zP-ULfLKTbEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', load_in_8bit=True, device_map='auto')\n","# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n","# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","# model = AutoModelForCausalLM.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n","\n","print(model)\n","print()\n","\n","config = BottleneckConfig(\n","    peft_type=\"BOTTLNECK\", target_modules=[\"c_fc\", \"c_proj\"],\n","    bottleneck_size=256, non_linearity=\"tanh\",\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = BottleneckModel(config, model)\n","\n","print(model)\n","# model = get_peft_model(model, prompt_config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["12aa776f8b054710863bb0bc89431cfd","d199cf8692fd41ef8b80ae860cc544d1","0343b9e9dee24bf3a50d090e9db44c2e","3d7c290b07314c27b75dfa5d70a093c0","04d3f34792db4fa8a2595e4f62cb7501","c58c9e1b33484261ac4a612d86d827b8","96a660e4b3324b589ce8a7f4acc11e38","3638e19f449d47609b92ddefe354d014","930163d3798c47b88185ebe2c85107ba","5ceb28a358c041b38316ec5c5ac25d48","8837218af4bd464bb3a573cad86830c4","4fe39b65b18748de833f8a6bbde95f5d","144de3bb77e940328f41262284dffc0a","7bcb95734fe24aa99966107b6410ec36","3dca42e95325463799d5b4410b01fc19","43821b3b9ce84512bd198ff3490d6334","60aa0b5991a0491ea652666517d8ba4d","209da89c0b1a49abab5db862d22e770c","7a324247c41e460c9dd0e4f7c2271611","7cf33e6bf07649bdbc46e603be0d7c17","29d629dc77ec4e9fbba82339ea3359f5","80082b5ab25240a88e0525edf6cbea95"]},"id":"pQ3zTslK1uLL","outputId":"751b083b-1501-4e5d-a8ce-2313d423a5c2","executionInfo":{"status":"ok","timestamp":1732597872125,"user_tz":300,"elapsed":4612,"user":{"displayName":"Lin John","userId":"00076252520359288091"}}},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12aa776f8b054710863bb0bc89431cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe39b65b18748de833f8a6bbde95f5d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")\n","\n","<bound method GPT2LMHeadModel.forward of GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=768, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")>\n","BottleneckModel(\n","  (model): GPT2LMHeadModel(\n","    (transformer): GPT2Model(\n","      (wte): Embedding(50257, 768)\n","      (wpe): Embedding(1024, 768)\n","      (drop): Dropout(p=0.1, inplace=False)\n","      (h): ModuleList(\n","        (0-11): 12 x GPT2Block(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): GPT2SdpaAttention(\n","            (c_attn): Conv1D(nf=2304, nx=768)\n","            (c_proj): Conv1D(nf=768, nx=768)\n","            (attn_dropout): Dropout(p=0.1, inplace=False)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): GPT2MLP(\n","            (c_fc): Conv1D(nf=768, nx=768)\n","            (c_proj): Conv1D(nf=768, nx=768)\n","            (act): NewGELUActivation()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["# Detailed parameter counting\n","total_params = 0\n","trainable_params = 0\n","non_trainable_params = 0\n","\n","for name, parameter in model.named_parameters():\n","    param_count = parameter.numel()\n","    total_params += param_count\n","\n","    if parameter.requires_grad:\n","        trainable_params += param_count\n","    else:\n","        non_trainable_params += param_count\n","\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n","print(f\"percentage: {(trainable_params/total_params)*100:.4f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNiIcY9t4Jyi","executionInfo":{"status":"ok","timestamp":1732597876490,"user_tz":300,"elapsed":284,"user":{"displayName":"Lin John","userId":"00076252520359288091"}},"outputId":"3936532a-d1f7-482f-d7e9-b584baf48428"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 138,632,448\n","Trainable parameters: 14,192,640\n","Non-trainable parameters: 124,439,808\n","percentage: 10.2376%\n"]}]},{"cell_type":"code","source":["# # model = LlamaForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', load_in_8bit=True, device_map='auto')\n","# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n","# # model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n","# # model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","# # model = AutoModelForCausalLM.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n","# # model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n","\n","# print(model)\n","# print()\n","\n","# config = BottleneckConfig(\n","#     peft_type=\"BOTTLNECK\", target_modules=[\"c_fc\", \"c_proj\"],\n","#     bottleneck_size=256, non_linearity=\"tanh\",\n","#     bias=\"none\",\n","#     task_type=\"CAUSAL_LM\",\n","# )\n","\n","# model = BottleneckModel(config, model)\n","\n","# print(model)\n","# # model = get_peft_model(model, prompt_config)"],"metadata":{"id":"wJoSF7AHGIGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","# Set the pad_token to the eos_token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Update model's config\n","model.config.pad_token_id = tokenizer.pad_token_id\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=100,\n","    # evaluation_strategy='steps',\n","    # eval_steps=1000,\n","    save_steps=3000,\n","    logging_dir='./logs',\n","    logging_steps=1000,\n","    learning_rate=5e-4,\n","    report_to='none',\n","    save_safetensors=False,\n",")\n","from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset=tokenized_dataset['validation'],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"],"metadata":{"id":"9HjKHsxpGWHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdzeDqJT5Yvu","outputId":"f46cc85c-3777-4501-d308-50e376f0d119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    776 MiB |  16870 MiB | 214361 GiB | 214360 GiB |\n","|       from large pool |    492 MiB |  16460 MiB | 212957 GiB | 212956 GiB |\n","|       from small pool |    283 MiB |    512 MiB |   1404 GiB |   1403 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    776 MiB |  16870 MiB | 214361 GiB | 214360 GiB |\n","|       from large pool |    492 MiB |  16460 MiB | 212957 GiB | 212956 GiB |\n","|       from small pool |    283 MiB |    512 MiB |   1404 GiB |   1403 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |    773 MiB |  16863 MiB | 214247 GiB | 214246 GiB |\n","|       from large pool |    490 MiB |  16453 MiB | 212843 GiB | 212843 GiB |\n","|       from small pool |    283 MiB |    512 MiB |   1403 GiB |   1403 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   2500 MiB |  17452 MiB |  52204 MiB |  49704 MiB |\n","|       from large pool |   2116 MiB |  16780 MiB |  50626 MiB |  48510 MiB |\n","|       from small pool |    384 MiB |    672 MiB |   1578 MiB |   1194 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |   1723 MiB |   3087 MiB |  76197 GiB |  76196 GiB |\n","|       from large pool |   1623 MiB |   2994 MiB |  74595 GiB |  74593 GiB |\n","|       from small pool |    100 MiB |    160 MiB |   1602 GiB |   1602 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     906    |    1658    |   10079 K  |   10078 K  |\n","|       from large pool |      52    |     311    |    5675 K  |    5675 K  |\n","|       from small pool |     854    |    1556    |    4404 K  |    4403 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     906    |    1658    |   10079 K  |   10078 K  |\n","|       from large pool |      52    |     311    |    5675 K  |    5675 K  |\n","|       from small pool |     854    |    1556    |    4404 K  |    4403 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     221    |     503    |    1500    |    1279    |\n","|       from large pool |      29    |     204    |     711    |     682    |\n","|       from small pool |     192    |     336    |     789    |     597    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     238    |     384    |    2987 K  |    2987 K  |\n","|       from large pool |      35    |      78    |    1785 K  |    1785 K  |\n","|       from small pool |     203    |     342    |    1202 K  |    1202 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"id":"bh0XGBoQGlV4","outputId":"ef1e6a37-f2fd-4352-b7d2-557a08c02687"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3537' max='3537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3537/3537 27:21, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.346400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.232000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.107400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3537, training_loss=2.2159470764971196, metrics={'train_runtime': 1641.6753, 'train_samples_per_second': 17.227, 'train_steps_per_second': 2.155, 'total_flos': 8622645620244480.0, 'train_loss': 2.2159470764971196, 'epoch': 3.0})"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["# Load the validation split\n","dataset = load_dataset(\"boolq\", split='validation')\n","formatted_dataset = dataset.map(format_example)"],"metadata":{"id":"lpmwvU8Lg1W0","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["5af359f57b7e4e17b22df4b5150c7956","97caa7ca30a244a792aa07508af13055","9edb51098178405f993d9c9a909aa962","2cb83798821740578c0fa40b197de17e","ea85af05241b4e7eb81870322cdd6d73","cbc2eb3712204d05b35ec232e5619a01","7ed370b5acef481d8cbb9d5c97b7c917","a8d062199bde4b10a742f824b1fee42a","73c0a807f9004250bb3d121da50827fe","792d03adbf324f5189c8f9f53b74d2d8","0493e8015bf14aefb3d0849c60869c2d"]},"outputId":"0b4bb70b-0b67-4650-d88e-741953c1205a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration boolq-150dc58bba7bf36d\n","WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/boolq-150dc58bba7bf36d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3270 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af359f57b7e4e17b22df4b5150c7956"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","model.eval()\n","\n","def generate_answer(example):\n","    input_text = example['input_text']\n","\n","    # Tokenize and move inputs to the model's device\n","    inputs = tokenizer(\n","        input_text,\n","        return_tensors='pt',\n","        max_length=512,\n","        truncation=True,\n","        padding='max_length',\n","    )\n","    input_ids = inputs['input_ids'].to(model.device)\n","    attention_mask = inputs['attention_mask'].to(model.device)\n","\n","    # Generate the model's output\n","    with torch.no_grad():\n","        output_ids = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            max_new_tokens=15,\n","            do_sample=False,\n","            num_beams=1,\n","            pad_token_id=tokenizer.eos_token_id,\n","        )\n","\n","    # Decode the generated tokens\n","    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","    # Extract the answer by removing the input prompt\n","    answer = generated_text[len(input_text):].strip()\n","\n","    # Handle empty outputs\n","    if not answer:\n","        answer = \"No\"\n","        return {'predicted_answer': answer}\n","\n","    # # Keep only the first word (Yes or No)\n","    # answer = answer.split()[0]\n","    # print(answer)\n","\n","    # # Handle unexpected outputs\n","    if 'yes' in answer.lower():\n","        answer = 'Yes'\n","    elif 'no' in answer.lower():\n","        answer = 'No'\n","    else:\n","        answer = 'No'\n","\n","    return {'predicted_answer': answer}\n","\n","\n","# Map the generate_answer function over the validation set\n","predictions = formatted_dataset.map(generate_answer, batched=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["a752d80586ae4ea19fabd6cf9396a535","5638110be76041f088ae104882670094","c16a16da07454db19a757b6aae8c85df","a6db3f71e92747b0b423aa696b47fd1d","78e48251d73a45c286cdb5d45e3190bf","48ead1e9cd38489aad3ea7fd606d5e44","ec0fb795924840918e00bd3ae49a87d4","1aea9dbd3c8849b18e9d470fff80b545","108e4c23437e4a8abfa08cf4de0c3df6","89255c8e73214fa8a2c3cd0c73114b76","748186de38604c5bbe033cb62c930abc"]},"id":"DHmOI7lsg892","outputId":"23a1358c-2b2b-496b-86d3-4f47dc2615c7","collapsed":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3270 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a752d80586ae4ea19fabd6cf9396a535"}},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","\n","def compute_accuracy(predictions):\n","    # Convert the true and predicted answers to lowercase for consistency\n","    true_answers = [answer.lower() for answer in predictions['target_text']]\n","    predicted_answers = [answer.lower() for answer in predictions['predicted_answer']]\n","\n","    # Calculate the number of correct predictions\n","    correct_predictions = sum([\n","        true == pred for true, pred in zip(true_answers, predicted_answers)\n","    ])\n","\n","    # Compute accuracy\n","    accuracy = correct_predictions / len(true_answers)\n","    return accuracy\n","\n","# Calculate accuracy\n","accuracy = compute_accuracy(predictions)\n","print(f\"Accuracy on the validation set: {accuracy * 100:.2f}%\")"],"metadata":{"id":"KvEk1vEMhH5E","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b58a59a-c552-46ba-fa31-eebadbc380c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on the validation set: 66.02%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RdalX9V0yXg6"},"execution_count":null,"outputs":[]}]}